{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query and Generate response \n",
    "- at top-k = 10 model seems to generate the wrong query string and failed to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test case 1/100: Battery b3c7 with feature 'slope_last_10_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar slope_last_10_cycles feature?'\n",
      "    Received response: b1c1, b1c3, b1c4, b1c0, b1c2, b1c41, b1c28, b1c24, b1c14, b1c29...\n",
      "  Processing question 2: 'Are there any batteries that match a slope_last_10_cycles feature?'\n",
      "    Received response: b1c1, b1c3, b1c4, b1c0, b1c2, b1c41, b1c28, b1c24, b1c14, b1c29...\n",
      "  Processing question 3: 'Can you locate batteries with a comparable slope_last_10_cycles feature?'\n",
      "    Received response: b1c0, b1c2, b1c41, b1c28, b1c24, b1c14, b1c29, b1c4, b1c3, b1c1...\n",
      "\n",
      "Processing test case 2/100: Battery b3c7 with feature 'mean_grad_last_10_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have similar mean_grad_last_10_cycles as this unknown one?'\n",
      "    Received response: b1c1, b1c3, b1c4, b1c0, b1c2, b1c41, b1c28, b1c24, b1c29, b1c14...\n",
      "  Processing question 2: 'Are there any batteries that match the mean_grad_last_10_cycles of this unidentified battery?'\n",
      "    Received response: b1c1, b1c3, b1c4, b1c0, b1c2, b1c41, b1c28, b1c24, b1c29, b1c14...\n",
      "  Processing question 3: 'Can you locate batteries with a similar mean_grad_last_10_cycles to this mystery battery?'\n",
      "    Received response: b1c1, b1c3, b1c4, b1c0, b1c2, b1c41, b1c28, b1c24, b1c29, b1c14...\n",
      "\n",
      "Processing test case 3/100: Battery b3c7 with feature 'slope_last_50_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar slope_last_50_cycles feature?'\n",
      "    Received response: b1c0, b1c1, b1c4, b1c2, b1c3, b1c14, b1c24, b1c29, b1c28, b1c41...\n",
      "  Processing question 2: 'Are there any batteries that match a slope_last_50_cycles feature?'\n",
      "Error invoking chain: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '|': expected an expression, 'FOREACH', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF> (line 1, column 130 (offset: 129))\n",
      "\"MATCH (b:Battery) RETURN b.battery_id, abs(b.slope_last_50_cycles - (-0.0004295814037322998)) as diff ORDER BY diff ASC LIMIT 10 | b1c1, b4c9, b9c1, b1c9, b8c4, b5c6, b3c7, b2c5, b7c3, b6c2\"\n",
      "                                                                                                                                  ^}. Returning empty response for scoring.\n",
      "    Received response: ...\n",
      "  Processing question 3: 'Can you locate batteries with a comparable slope_last_50_cycles characteristic?'\n",
      "    Received response: b1c41, b1c28, b1c29, b1c14, b1c24, b1c0, b1c1, b1c4, b1c3, b1c2...\n",
      "\n",
      "Processing test case 4/100: Battery b3c7 with feature 'mean_grad_last_50_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar mean_grad_last_50_cycles feature?'\n",
      "    Received response: b1c3, b1c2, b1c4, b1c1, b1c0, b1c41, b1c28, b1c29, b1c14, b1c24...\n",
      "  Processing question 2: 'Are there any batteries that share the same mean_grad_last_50_cycles characteristic?'\n",
      "    Received response: b1c41, b1c28, b1c29, b1c14, b1c24, b1c0, b1c1, b1c4, b1c3, b1c2...\n",
      "  Processing question 3: 'Can you locate batteries with a comparable mean_grad_last_50_cycles attribute?'\n",
      "    Received response: b1c3, b1c2, b1c4, b1c1, b1c0, b1c41, b1c28, b1c29, b1c14, b1c24...\n",
      "\n",
      "Processing test case 5/100: Battery b3c7 with feature 'slope_last_100_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar slope_last_100_cycles feature?'\n",
      "Error invoking chain: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '|': expected an expression, 'FOREACH', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF> (line 1, column 131 (offset: 130))\n",
      "\"MATCH (b:Battery) RETURN b.battery_id, abs(b.slope_last_100_cycles - (-0.0004307425022125244)) as diff ORDER BY diff ASC LIMIT 10 | b1c1, b4c7, b9c3, b13c9, b17c5, b2c11, b5c8, b7c2, b11c6, b15c10\"\n",
      "                                                                                                                                   ^}. Returning empty response for scoring.\n",
      "    Received response: ...\n",
      "  Processing question 2: 'Are there any batteries that match the slope_last_100_cycles feature of this unknown battery?'\n",
      "    Received response: b1c0, b1c1, b1c4, b1c3, b1c2, b1c24, b1c41, b1c28, b1c29, b1c20...\n",
      "  Processing question 3: 'Can you locate batteries with a comparable slope_last_100_cycles characteristic?'\n",
      "    Received response: b1c3, b1c2, b1c4, b1c1, b1c0, b1c24, b1c41, b1c28, b1c29, b1c20...\n",
      "\n",
      "Processing test case 6/100: Battery b3c7 with feature 'mean_grad_last_100_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar mean_grad_last_100_cycles feature?'\n",
      "    Received response: b1c3, b1c2, b1c4, b1c0, b1c1, b1c24, b1c41, b1c28, b1c29, b1c20...\n",
      "  Processing question 2: 'Are there any batteries that share the same mean_grad_last_100_cycles characteristic?'\n",
      "    Received response: b1c24, b1c41, b1c28, b1c29, b1c20, b1c1, b1c0, b1c4, b1c3, b1c2...\n",
      "  Processing question 3: 'Can you locate batteries with a comparable mean_grad_last_100_cycles attribute?'\n",
      "    Received response: b1c3, b1c2, b1c4, b1c0, b1c1, b1c24, b1c41, b1c28, b1c29, b1c20...\n",
      "\n",
      "Processing test case 7/100: Battery b3c7 with feature 'slope_last_200_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar slope_last_200_cycles feature as this one?'\n",
      "    Received response: b1c4, b1c3, b1c2, b1c1, b1c0, b1c9, b1c20, b1c24, b1c40, b1c41...\n",
      "  Processing question 2: 'Are there any batteries that match this one in terms of the slope_last_200_cycles feature?'\n",
      "    Received response: b1c4, b1c3, b1c2, b1c1, b1c0, b1c20, b1c24, b1c41, b1c40, b1c9...\n",
      "  Processing question 3: 'Can you locate batteries with a slope_last_200_cycles feature that is comparable to this one?'\n",
      "    Received response: b1c4, b1c3, b1c2, b1c1, b1c0, b1c9, b1c20, b1c24, b1c41, b1c40...\n",
      "\n",
      "Processing test case 8/100: Battery b3c7 with feature 'mean_grad_last_200_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar mean_grad_last_200_cycles feature?'\n",
      "    Received response: b1c4, b1c3, b1c2, b1c1, b1c0, b1c9, b1c20, b1c24, b1c41, b1c40...\n",
      "  Processing question 2: 'Are there any batteries that share the same mean_grad_last_200_cycles characteristic?'\n",
      "    Received response: b1c4, b1c3, b1c2, b1c1, b1c0, b1c9, b1c20, b1c24, b1c40, b1c41...\n",
      "  Processing question 3: 'Can you locate batteries with a comparable mean_grad_last_200_cycles attribute?'\n",
      "    Received response: b1c4, b1c3, b1c2, b1c1, b1c0, b1c9, b1c20, b1c24, b1c41, b1c40...\n",
      "\n",
      "Processing test case 9/100: Battery b3c7 with feature 'slope_last_300_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar slope_last_300_cycles feature?'\n",
      "    Received response: b1c4, b1c1, b1c2, b1c3, b1c0, b1c5, b1c20, b1c18, b1c24, b1c41...\n",
      "  Processing question 2: 'Are there any batteries that match the slope_last_300_cycles feature of this unknown battery?'\n",
      "    Received response: b1c4, b1c2, b1c1, b1c3, b1c0, b1c20, b1c18, b1c24, b1c41, b1c5...\n",
      "  Processing question 3: 'Can you locate batteries with a comparable slope_last_300_cycles characteristic?'\n",
      "Error invoking chain: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '|': expected an expression, 'FOREACH', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF> (line 1, column 132 (offset: 131))\n",
      "\"MATCH (b:Battery) RETURN b.battery_id, abs(b.slope_last_300_cycles - (-0.00035009324550628663)) as diff ORDER BY diff ASC LIMIT 10 | b1c1, b2c5, b8c9, b4c1, b9c7, b1c9, b7c3, b6c2, b3c8, b5c6\"\n",
      "                                                                                                                                    ^}. Returning empty response for scoring.\n",
      "    Received response: ...\n",
      "\n",
      "Processing test case 10/100: Battery b3c7 with feature 'mean_grad_last_300_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have similar mean_grad_last_300_cycles as this unknown battery?'\n",
      "    Received response: b1c4, b1c2, b1c1, b1c3, b1c0, b1c20, b1c18, b1c24, b1c41, b1c5...\n",
      "  Processing question 2: 'Are there any batteries that match the mean_grad_last_300_cycles of this particular battery?'\n",
      "    Received response: b1c4, b1c2, b1c1, b1c3, b1c0, b1c20, b1c18, b1c24, b1c41, b1c5...\n",
      "  Processing question 3: 'Could you locate batteries with a similar mean_grad_last_300_cycles to this unknown battery?'\n",
      "    Received response: b1c4, b1c2, b1c1, b1c3, b1c0, b1c20, b1c18, b1c24, b1c41, b1c5...\n",
      "\n",
      "Processing test case 11/100: Battery b3c7 with feature 'slope_last_400_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar slope_last_400_cycles feature?'\n",
      "    Received response: b1c4, b1c1, b1c2, b1c3, b1c0, b1c18, b1c5, b1c11, b1c24, b1c41...\n",
      "  Processing question 2: 'Are there any batteries that match a slope_last_400_cycles feature?'\n",
      "Error invoking chain: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '|': expected an expression, 'FOREACH', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF> (line 1, column 131 (offset: 130))\n",
      "\"MATCH (b:Battery) RETURN b.battery_id, abs(b.slope_last_400_cycles - (-0.0003128772974014282)) as diff ORDER BY diff ASC LIMIT 10 | b1c1, b5c8, b9c3, b1c9, b8c1, b5c2, b9c6, b1c4, b8c9, b5c5\"\n",
      "                                                                                                                                   ^}. Returning empty response for scoring.\n",
      "    Received response: ...\n",
      "  Processing question 3: 'Can you locate batteries with a comparable slope_last_400_cycles feature?'\n",
      "    Received response: b1c4, b1c1, b1c2, b1c3, b1c0, b1c18, b1c5, b1c11, b1c24, b1c41...\n",
      "\n",
      "Processing test case 12/100: Battery b3c7 with feature 'mean_grad_last_400_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have similar mean_grad_last_400_cycles as this unknown one?'\n",
      "    Received response: b1c4, b1c1, b1c2, b1c3, b1c0, b1c18, b1c5, b1c11, b1c24, b1c41...\n",
      "  Processing question 2: 'Are there any batteries that match the mean_grad_last_400_cycles of this unidentified battery?'\n",
      "    Received response: b1c4, b1c1, b1c2, b1c3, b1c0, b1c18, b1c5, b1c11, b1c24, b1c41...\n",
      "  Processing question 3: 'Can you locate batteries with a similar mean_grad_last_400_cycles to this mystery battery?'\n",
      "    Received response: b1c4, b1c1, b1c2, b1c3, b1c0, b1c18, b1c5, b1c11, b1c24, b1c41...\n",
      "\n",
      "Processing test case 13/100: Battery b3c7 with feature 'slope_last_500_cycles'\n",
      "  Processing question 1: 'Can you identify batteries that have a similar slope_last_500_cycles feature?'\n",
      "    Received response: b1c4, b1c1, b1c2, b1c0, b1c3, b1c11, b1c18, b1c5, b1c24, b1c41...\n",
      "  Processing question 2: 'Are there any batteries that match the slope_last_500_cycles feature of this unknown battery?'\n",
      "    Received response: b1c4, b1c1, b1c2, b1c0, b1c3, b1c11, b1c18, b1c5, b1c24, b1c41...\n",
      "  Processing question 3: 'Can you locate batteries with a comparable slope_last_500_cycles characteristic?'\n",
      "    Received response: b1c4, b1c1, b1c2, b1c0, b1c3, b1c11, b1c18, b1c5, b1c24, b1c41...\n",
      "\n",
      "Processing test case 14/100: Battery b3c7 with feature 'mean_grad_last_500_cycles'\n",
      "  Processing question 1: 'Are there any batteries that have similar mean_grad_last_500_cycles as this unknown one?'\n",
      "Error invoking chain: {code: Neo.ClientError.Statement.SyntaxError} {message: Invalid input '|': expected an expression, 'FOREACH', 'ORDER BY', 'CALL', 'CREATE', 'LOAD CSV', 'DELETE', 'DETACH', 'FINISH', 'INSERT', 'LIMIT', 'MATCH', 'MERGE', 'NODETACH', 'OFFSET', 'OPTIONAL', 'REMOVE', 'RETURN', 'SET', 'SKIP', 'UNION', 'UNWIND', 'USE', 'WITH' or <EOF> (line 1, column 135 (offset: 134))\n",
      "\"MATCH (b:Battery) RETURN b.battery_id, abs(b.mean_grad_last_500_cycles - (-0.0002763822078704834)) as diff ORDER BY diff ASC LIMIT 10 | b1c1, b3c10, b5c2, b9c8, b11c6, b2c4, b7c9, b1c3, b6c5, b10c7\"\n",
      "                                                                                                                                       ^}. Returning empty response for scoring.\n",
      "    Received response: ...\n",
      "\n",
      "LLM responses saved to /home/jaf/battery-lifespan-kg/eval/llm_response_for_evaluator_question.csv.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m         output_df\u001b[38;5;241m.\u001b[39mto_csv(output_csv_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, quoting\u001b[38;5;241m=\u001b[39mcsv\u001b[38;5;241m.\u001b[39mQUOTE_ALL)\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLLM responses saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[43msave_llm_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 91\u001b[0m, in \u001b[0;36msave_llm_responses\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m             row_data[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_raw_response\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m raw_response\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;66;03m# Optional delay between questions to avoid rate limiting\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m             \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m         output_rows\u001b[38;5;241m.\u001b[39mappend(row_data)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m global_error:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# Import the pipeline that calls the LLM\n",
    "from utils.pipeline import run_pipeline\n",
    "\n",
    "# Configurable parameter: top_k (must be less than 15)\n",
    "TOP_K = 5  # Adjust as needed\n",
    "\n",
    "# Paths\n",
    "TESTSET_CSV_PATH = \"/home/jaf/battery-lifespan-kg/eval/evaluator_question.csv\"  # Update if needed\n",
    "BATTERY_FILES_DIR = \"/home/jaf/battery-lifespan-kg/resources/testset\"\n",
    "OUTPUT_DIR = \"/home/jaf/battery-lifespan-kg/eval\"\n",
    "\n",
    "def save_llm_responses():\n",
    "    # Read the testset CSV\n",
    "    df = pd.read_csv(TESTSET_CSV_PATH)\n",
    "    \n",
    "    # Identify and sort sample question columns (e.g., SAMPLE_QUESTION_1, SAMPLE_QUESTION_2, ...)\n",
    "    sample_question_cols = sorted(\n",
    "        [col for col in df.columns if col.startswith(\"SAMPLE_QUESTION\")],\n",
    "        key=lambda x: int(x.split('_')[-1])\n",
    "    )\n",
    "    \n",
    "    output_rows = []\n",
    "    total_cases = len(df)\n",
    "    \n",
    "    try:\n",
    "        # Iterate over each test case\n",
    "        for idx, row in df.iterrows():\n",
    "            test_battery_id = row[\"TEST_BATTERY_ID\"]\n",
    "            test_battery_query_feature = row[\"TEST_BATTERY_QUERY_FEATURE\"]\n",
    "            print(f\"\\nProcessing test case {idx+1}/{total_cases}: Battery {test_battery_id} with feature '{test_battery_query_feature}'\")\n",
    "            battery_file_path = os.path.join(BATTERY_FILES_DIR, f\"{test_battery_id}.txt\")\n",
    "            \n",
    "            # Read the battery file content\n",
    "            try:\n",
    "                with open(battery_file_path, \"r\") as f:\n",
    "                    file_content = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file for battery {test_battery_id}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare the row data dictionary with base columns\n",
    "            row_data = {\n",
    "                \"TEST_BATTERY_ID\": test_battery_id,\n",
    "                \"TEST_BATTERY_QUERY_FEATURE\": test_battery_query_feature\n",
    "            }\n",
    "            \n",
    "            # For each sample question, obtain the raw LLM response\n",
    "            for i, question_col in enumerate(sample_question_cols, start=1):\n",
    "                base_question = str(row[question_col]).strip()\n",
    "                print(f\"  Processing question {i}: {base_question}\")\n",
    "                \n",
    "                # Improved instruction:\n",
    "                # Ask the LLM to output exactly a comma-separated list of battery IDs, with no additional text.\n",
    "                modified_question = (\n",
    "                    f\"{base_question} \"\n",
    "                    f\"Return exactly a comma-separated list of the top-{TOP_K} battery IDs from the most similar batteries. \"\n",
    "                    \"Do not include any explanation or extra text. Each battery ID must be in the format 'b<number>c<number>' \"\n",
    "                    \"and the order should reflect ranking from most similar to least. For example, if the top-5 batteries are \"\n",
    "                    \"b1c1, b1c2, b1c3, b1c4, and b1c5, then the response should be: b1c1, b1c2, b1c3, b1c4, b1c5. \"\n",
    "                    \"Do not add any additional characters or text.\"\n",
    "                )\n",
    "\n",
    "                \n",
    "                # Create a new file-like object for each call (to avoid pointer issues)\n",
    "                uploaded_file = StringIO(file_content)\n",
    "                \n",
    "                try:\n",
    "                    # Run the pipeline with the modified question and the file content\n",
    "                    response = run_pipeline(modified_question, uploaded_file)\n",
    "                    if isinstance(response, str):\n",
    "                        raw_response = response\n",
    "                    else:\n",
    "                        raw_response = response.get(\"result\", \"\")\n",
    "                    print(f\"    Received response: {raw_response[:100]}...\")  # Print first 50 characters\n",
    "                except Exception as e:\n",
    "                    raw_response = f\"API error: {str(e)}\"\n",
    "                    print(f\"    Error processing battery {test_battery_id} question '{base_question}': {str(e)}\")\n",
    "                \n",
    "                # Save the original question and its raw response in the row data\n",
    "                row_data[f\"q{i}\"] = base_question\n",
    "                row_data[f\"q{i}_raw_response\"] = raw_response\n",
    "                \n",
    "                # Optional delay between questions to avoid rate limiting\n",
    "                time.sleep(1)\n",
    "            \n",
    "            output_rows.append(row_data)\n",
    "    except Exception as global_error:\n",
    "        print(f\"\\nA global error occurred: {str(global_error)}\")\n",
    "    finally:\n",
    "        # Create a DataFrame from the collected rows even if an error occurred\n",
    "        output_df = pd.DataFrame(output_rows)\n",
    "        \n",
    "        # Construct the output CSV path using the input CSV name\n",
    "        input_csv_name = os.path.basename(TESTSET_CSV_PATH)\n",
    "        output_csv_path = os.path.join(OUTPUT_DIR, f\"llm_response_for_{input_csv_name}.csv\")\n",
    "        \n",
    "        # Clean raw response columns to replace newlines with spaces and escape quotes\n",
    "        for col in output_df.columns:\n",
    "            if col.endswith(\"_raw_response\"):\n",
    "                output_df[col] = output_df[col].str.replace('\\n', ' ', regex=False)\n",
    "                output_df[col] = output_df[col].str.replace('\"', '\"\"', regex=False)\n",
    "        \n",
    "        # Write CSV with all fields quoted to protect against commas within fields\n",
    "        output_df.to_csv(output_csv_path, index=False, quoting=csv.QUOTE_ALL)\n",
    "        print(f\"\\nLLM responses saved to {output_csv_path}\")\n",
    "\n",
    "save_llm_responses()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Top-5 Accuracy: 83.00%\n",
      "Mean Reciprocal Rank (MRR): 0.382\n",
      "Precision@5: 1.000\n",
      "Recall@5: 0.830\n",
      "F1 Score: 0.907\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Configurable parameter\n",
    "TOP_K = 5  # Change this to evaluate top-k\n",
    "\n",
    "def extract_battery_ids(text):\n",
    "    \"\"\"\n",
    "    Extract battery IDs matching the pattern: b{number}c{number}\n",
    "    (returns a list in left-to-right order)\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\bb\\d+c\\d+\\b', text, flags=re.IGNORECASE)\n",
    "\n",
    "def evaluate_query(ground_truth, pred_text, k):\n",
    "    \"\"\"\n",
    "    For a single query, extract predicted battery IDs from the LLM response,\n",
    "    ensure we have at least k predictions (pad with empty strings if needed),\n",
    "    and then evaluate:\n",
    "      - Binary outcome: 1 if the ground truth appears in the top-k, else 0.\n",
    "      - Reciprocal Rank (RR): 1/(rank) if found; otherwise 0.\n",
    "    \"\"\"\n",
    "    preds = extract_battery_ids(pred_text)\n",
    "    # Pad with empty strings if fewer than k predictions\n",
    "    if len(preds) < k:\n",
    "        preds += [\"\"] * (k - len(preds))\n",
    "    found = 0\n",
    "    rr = 0\n",
    "    for i, p in enumerate(preds[:k]):\n",
    "        if p.strip().lower() == ground_truth.strip().lower():\n",
    "            found = 1\n",
    "            rr = 1.0 / (i + 1)\n",
    "            break\n",
    "    return found, rr\n",
    "\n",
    "# Load ground truth CSV and predicted responses CSV\n",
    "gt_df = pd.read_csv(\"/home/jaf/battery-lifespan-kg/eval/evaluator_question.csv\")\n",
    "pred_df = pd.read_csv(\"/home/jaf/battery-lifespan-kg/eval/llm_response_for_evaluator_question.csv.csv\")\n",
    "\n",
    "# Merge the two dataframes based on test battery ID and query feature\n",
    "merged_df = pd.merge(gt_df, pred_df, on=[\"TEST_BATTERY_ID\", \"TEST_BATTERY_QUERY_FEATURE\"], how=\"inner\")\n",
    "\n",
    "# We assume there are three sample questions per row.\n",
    "num_questions = 3\n",
    "binary_labels = []  # 1 if correct (ground truth in top-k), else 0\n",
    "rr_list = []       # Reciprocal Rank for each query\n",
    "\n",
    "# Loop through each test case and each question\n",
    "for idx, row in merged_df.iterrows():\n",
    "    for i in range(1, num_questions + 1):\n",
    "        gt_col = f\"{i}_Most_Similar_Battery_ID\"\n",
    "        pred_col = f\"q{i}_raw_response\"\n",
    "        if pd.isna(row.get(gt_col)):\n",
    "            continue  # Skip if ground truth is not available\n",
    "        ground_truth = str(row[gt_col])\n",
    "        pred_text = str(row.get(pred_col, \"\"))\n",
    "        label, rr = evaluate_query(ground_truth, pred_text, TOP_K)\n",
    "        binary_labels.append(label)\n",
    "        rr_list.append(rr)\n",
    "\n",
    "# For our binary evaluation, ground truth labels for each query are all 1 (we expect it to be found)\n",
    "y_true = [1] * len(binary_labels)\n",
    "y_pred = binary_labels\n",
    "\n",
    "# Use scikit-learn to compute metrics\n",
    "topk_accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "mrr = sum(rr_list) / len(rr_list) if rr_list else 0\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Top-{} Accuracy: {:.2f}%\".format(TOP_K, topk_accuracy * 100))\n",
    "print(\"Mean Reciprocal Rank (MRR): {:.3f}\".format(mrr))\n",
    "print(\"Precision@{}: {:.3f}\".format(TOP_K, precision))\n",
    "print(\"Recall@{}: {:.3f}\".format(TOP_K, recall))\n",
    "print(\"F1 Score: {:.3f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-feature evaluation metrics:\n",
      "\n",
      "Feature: cycle\n",
      "  Top-1 Accuracy: 25.00%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.250\n",
      "  F1 Score: 0.400\n",
      "  Mean Reciprocal Rank (MRR): 0.250\n",
      "\n",
      "Feature: mean_grad_last_1000_cycles\n",
      "  Top-1 Accuracy: 25.00%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.250\n",
      "  F1 Score: 0.400\n",
      "  Mean Reciprocal Rank (MRR): 0.250\n",
      "\n",
      "Feature: mean_grad_last_100_cycles\n",
      "  Top-1 Accuracy: 8.33%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.083\n",
      "  F1 Score: 0.154\n",
      "  Mean Reciprocal Rank (MRR): 0.083\n",
      "\n",
      "Feature: mean_grad_last_10_cycles\n",
      "  Top-1 Accuracy: 16.67%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.167\n",
      "  F1 Score: 0.286\n",
      "  Mean Reciprocal Rank (MRR): 0.167\n",
      "\n",
      "Feature: mean_grad_last_200_cycles\n",
      "  Top-1 Accuracy: 16.67%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.167\n",
      "  F1 Score: 0.286\n",
      "  Mean Reciprocal Rank (MRR): 0.167\n",
      "\n",
      "Feature: mean_grad_last_300_cycles\n",
      "  Top-1 Accuracy: 16.67%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.167\n",
      "  F1 Score: 0.286\n",
      "  Mean Reciprocal Rank (MRR): 0.167\n",
      "\n",
      "Feature: mean_grad_last_400_cycles\n",
      "  Top-1 Accuracy: 8.33%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.083\n",
      "  F1 Score: 0.154\n",
      "  Mean Reciprocal Rank (MRR): 0.083\n",
      "\n",
      "Feature: mean_grad_last_500_cycles\n",
      "  Top-1 Accuracy: 16.67%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.167\n",
      "  F1 Score: 0.286\n",
      "  Mean Reciprocal Rank (MRR): 0.167\n",
      "\n",
      "Feature: mean_grad_last_50_cycles\n",
      "  Top-1 Accuracy: 16.67%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.167\n",
      "  F1 Score: 0.286\n",
      "  Mean Reciprocal Rank (MRR): 0.167\n",
      "\n",
      "Feature: mean_grad_last_600_cycles\n",
      "  Top-1 Accuracy: 33.33%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.333\n",
      "  F1 Score: 0.500\n",
      "  Mean Reciprocal Rank (MRR): 0.333\n",
      "\n",
      "Feature: mean_grad_last_700_cycles\n",
      "  Top-1 Accuracy: 25.00%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.250\n",
      "  F1 Score: 0.400\n",
      "  Mean Reciprocal Rank (MRR): 0.250\n",
      "\n",
      "Feature: mean_grad_last_800_cycles\n",
      "  Top-1 Accuracy: 16.67%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.167\n",
      "  F1 Score: 0.286\n",
      "  Mean Reciprocal Rank (MRR): 0.167\n",
      "\n",
      "Feature: mean_grad_last_900_cycles\n",
      "  Top-1 Accuracy: 25.00%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.250\n",
      "  F1 Score: 0.400\n",
      "  Mean Reciprocal Rank (MRR): 0.250\n",
      "\n",
      "Feature: slope_last_1000_cycles\n",
      "  Top-1 Accuracy: 25.00%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.250\n",
      "  F1 Score: 0.400\n",
      "  Mean Reciprocal Rank (MRR): 0.250\n",
      "\n",
      "Feature: slope_last_100_cycles\n",
      "  Top-1 Accuracy: 16.67%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.167\n",
      "  F1 Score: 0.286\n",
      "  Mean Reciprocal Rank (MRR): 0.167\n",
      "\n",
      "Feature: slope_last_10_cycles\n",
      "  Top-1 Accuracy: 33.33%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.333\n",
      "  F1 Score: 0.500\n",
      "  Mean Reciprocal Rank (MRR): 0.333\n",
      "\n",
      "Feature: slope_last_200_cycles\n",
      "  Top-1 Accuracy: 8.33%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.083\n",
      "  F1 Score: 0.154\n",
      "  Mean Reciprocal Rank (MRR): 0.083\n",
      "\n",
      "Feature: slope_last_300_cycles\n",
      "  Top-1 Accuracy: 25.00%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.250\n",
      "  F1 Score: 0.400\n",
      "  Mean Reciprocal Rank (MRR): 0.250\n",
      "\n",
      "Feature: slope_last_400_cycles\n",
      "  Top-1 Accuracy: 25.00%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.250\n",
      "  F1 Score: 0.400\n",
      "  Mean Reciprocal Rank (MRR): 0.250\n",
      "\n",
      "Feature: slope_last_500_cycles\n",
      "  Top-1 Accuracy: 8.33%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.083\n",
      "  F1 Score: 0.154\n",
      "  Mean Reciprocal Rank (MRR): 0.083\n",
      "\n",
      "Feature: slope_last_50_cycles\n",
      "  Top-1 Accuracy: 8.33%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.083\n",
      "  F1 Score: 0.154\n",
      "  Mean Reciprocal Rank (MRR): 0.083\n",
      "\n",
      "Feature: slope_last_600_cycles\n",
      "  Top-1 Accuracy: 8.33%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.083\n",
      "  F1 Score: 0.154\n",
      "  Mean Reciprocal Rank (MRR): 0.083\n",
      "\n",
      "Feature: slope_last_700_cycles\n",
      "  Top-1 Accuracy: 8.33%\n",
      "  Precision@1: 1.000\n",
      "  Recall@1: 0.083\n",
      "  F1 Score: 0.154\n",
      "  Mean Reciprocal Rank (MRR): 0.083\n",
      "\n",
      "Feature: slope_last_800_cycles\n",
      "  Top-1 Accuracy: 0.00%\n",
      "  Precision@1: 0.000\n",
      "  Recall@1: 0.000\n",
      "  F1 Score: 0.000\n",
      "  Mean Reciprocal Rank (MRR): 0.000\n",
      "\n",
      "Feature: slope_last_900_cycles\n",
      "  Top-1 Accuracy: 0.00%\n",
      "  Precision@1: 0.000\n",
      "  Recall@1: 0.000\n",
      "  F1 Score: 0.000\n",
      "  Mean Reciprocal Rank (MRR): 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Configurable parameter\n",
    "TOP_K = 1  # Evaluate top-1; set to 3 if you want top-3, etc.\n",
    "\n",
    "def extract_battery_ids(text):\n",
    "    \"\"\"\n",
    "    Extract battery IDs matching the pattern b{number}c{number} in left-to-right order.\n",
    "    For example, from \"b1c1, b1c3\" it returns [\"b1c1\", \"b1c3\"].\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\bb\\d+c\\d+\\b', text, flags=re.IGNORECASE)\n",
    "\n",
    "def evaluate_query(ground_truth, pred_text, k):\n",
    "    \"\"\"\n",
    "    For a single query:\n",
    "      - Extract battery IDs from the predicted text.\n",
    "      - If fewer than k predictions are returned, pad with empty strings.\n",
    "      - Check if the ground truth is within the top k predictions.\n",
    "      - Calculate the reciprocal rank (1/(rank position)) if found, or 0 if not.\n",
    "      \n",
    "    Returns:\n",
    "      found (binary: 1 if correct answer is found in top-k, else 0),\n",
    "      rr (reciprocal rank)\n",
    "    \"\"\"\n",
    "    preds = extract_battery_ids(pred_text)\n",
    "    if len(preds) < k:\n",
    "        preds += [\"\"] * (k - len(preds))\n",
    "    found = 0\n",
    "    rr = 0\n",
    "    for idx, p in enumerate(preds[:k]):\n",
    "        if p.strip().lower() == ground_truth.strip().lower():\n",
    "            found = 1\n",
    "            rr = 1.0 / (idx + 1)\n",
    "            break\n",
    "    return found, rr\n",
    "\n",
    "# Load the ground truth and predicted responses CSV files.\n",
    "gt_df = pd.read_csv(\"/home/jaf/battery-lifespan-kg/eval/evaluator_question.csv\")\n",
    "pred_df = pd.read_csv(\"/home/jaf/battery-lifespan-kg/eval/llm_response_for_evaluator_question.csv.csv\")\n",
    "\n",
    "# Merge dataframes on TEST_BATTERY_ID and TEST_BATTERY_QUERY_FEATURE.\n",
    "merged_df = pd.merge(gt_df, pred_df, on=[\"TEST_BATTERY_ID\", \"TEST_BATTERY_QUERY_FEATURE\"], how=\"inner\")\n",
    "\n",
    "# We'll assume there are three sample questions per row.\n",
    "num_questions = 3\n",
    "\n",
    "# Collect metrics for each query along with its feature.\n",
    "records = []  # Each record: feature, binary label, and reciprocal rank (rr)\n",
    "\n",
    "for idx, row in merged_df.iterrows():\n",
    "    feature = row[\"TEST_BATTERY_QUERY_FEATURE\"]\n",
    "    for i in range(1, num_questions + 1):\n",
    "        gt_col = f\"{i}_Most_Similar_Battery_ID\"  # Ground truth column for query i\n",
    "        pred_col = f\"q{i}_raw_response\"            # LLM response column for query i\n",
    "        if pd.isna(row.get(gt_col)):\n",
    "            continue  # Skip if ground truth is missing\n",
    "        ground_truth = str(row[gt_col])\n",
    "        pred_text = str(row.get(pred_col, \"\"))\n",
    "        label, rr = evaluate_query(ground_truth, pred_text, TOP_K)\n",
    "        records.append({\n",
    "            \"feature\": feature,\n",
    "            \"label\": label,\n",
    "            \"rr\": rr\n",
    "        })\n",
    "\n",
    "# Convert records to a DataFrame.\n",
    "eval_df = pd.DataFrame(records)\n",
    "\n",
    "# Now, compute and print metrics for each feature.\n",
    "print(\"Per-feature evaluation metrics:\\n\")\n",
    "for feature, group in eval_df.groupby(\"feature\"):\n",
    "    y_true = [1] * len(group)  # We expect the ground truth to be present in each query.\n",
    "    y_pred = group[\"label\"].tolist()\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mrr = group[\"rr\"].mean()\n",
    "    \n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(f\"  Top-{TOP_K} Accuracy: {acc * 100:.2f}%\")\n",
    "    print(f\"  Precision@{TOP_K}: {prec:.3f}\")\n",
    "    print(f\"  Recall@{TOP_K}: {rec:.3f}\")\n",
    "    print(f\"  F1 Score: {f1:.3f}\")\n",
    "    print(f\"  Mean Reciprocal Rank (MRR): {mrr:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tserie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
