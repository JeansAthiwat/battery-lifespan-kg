{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Import the pipeline that calls the LLM\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_pipeline\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Configurable parameter: top_k (must be less than 15)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m TOP_K \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n",
      "File \u001b[0;32m~/battery-lifespan-kg/utils/pipeline.py:42\u001b[0m\n\u001b[1;32m     39\u001b[0m embedder \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(openai_api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# --- Initialize Neo4j Vector Store for Semantic Similarity ---\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mNeo4jVector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNEO4J_URI\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43musername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNEO4J_USERNAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNEO4J_PASSWORD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedder\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# --- (Optional) Example Prompts for Battery Data ---\u001b[39;00m\n\u001b[1;32m     50\u001b[0m examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     51\u001b[0m     {\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich battery has the highest total cycles?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# ... add additional examples as needed ...\u001b[39;00m\n\u001b[1;32m     56\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/langchain_neo4j/vectorstores/neo4j_vector.py:604\u001b[0m, in \u001b[0;36mNeo4jVector.__init__\u001b[0;34m(self, embedding, search_type, username, password, url, keyword_index_name, database, index_name, node_label, embedding_node_property, text_node_property, distance_strategy, logger, pre_delete_collection, retrieval_query, relevance_score_fn, index_type, graph, embedding_dimension)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dimension \u001b[38;5;241m=\u001b[39m embedding_dimension\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;66;03m# Calculate embedding dimension\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# Delete existing data if flagged\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_delete_collection:\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:700\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m        Embedding for the text.\u001b[39;00m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:671\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:497\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    495\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 497\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43membed_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    503\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/langchain_community/embeddings/openai.py:120\u001b[0m, in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/openai/resources/embeddings.py:125\u001b[0m, in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     opts = FinalRequestOptions.construct(method=\"delete\", url=path, json_data=body, **options)\n\u001b[1;32m   1278\u001b[0m     return self.request(cast_to, opts)\n\u001b[1;32m   1280\u001b[0m def get_api_list(\n\u001b[1;32m   1281\u001b[0m     self,\n\u001b[1;32m   1282\u001b[0m     path: str,\n\u001b[0;32m-> 1283\u001b[0m     *,\n\u001b[1;32m   1284\u001b[0m     model: Type[object],\n\u001b[1;32m   1285\u001b[0m     page: Type[SyncPageT],\n\u001b[1;32m   1286\u001b[0m     body: Body | None = None,\n\u001b[1;32m   1287\u001b[0m     options: RequestOptions = {},\n\u001b[1;32m   1288\u001b[0m     method: str = \"get\",\n\u001b[1;32m   1289\u001b[0m ) -> SyncPageT:\n\u001b[1;32m   1290\u001b[0m     opts = FinalRequestOptions.construct(method=method, url=path, json_data=body, **options)\n\u001b[1;32m   1291\u001b[0m     return self._request_api_list(model, page, opts)\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    956\u001b[0m         request,\n\u001b[1;32m    957\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    959\u001b[0m     )\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    961\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI Response types must subclass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAPIResponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morigin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m     response_cls \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype[BaseAPIResponse[Any]]\u001b[39m\u001b[38;5;124m\"\u001b[39m, cast_to)\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m   1097\u001b[0m         ResponseT,\n\u001b[0;32m-> 1098\u001b[0m         response_cls(\n\u001b[1;32m   1099\u001b[0m             raw\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m   1100\u001b[0m             client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1101\u001b[0m             cast_to\u001b[38;5;241m=\u001b[39mextract_response_type(response_cls),\n\u001b[1;32m   1102\u001b[0m             stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1103\u001b[0m             stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1104\u001b[0m             options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1105\u001b[0m             retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1106\u001b[0m         ),\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_to \u001b[38;5;241m==\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, response)\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/openai/_base_client.py:1049\u001b[0m, in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/openai/_base_client.py:1098\u001b[0m, in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI Response types must subclass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAPIResponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morigin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m     response_cls \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype[BaseAPIResponse[Any]]\u001b[39m\u001b[38;5;124m\"\u001b[39m, cast_to)\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m   1097\u001b[0m         ResponseT,\n\u001b[0;32m-> 1098\u001b[0m         response_cls(\n\u001b[1;32m   1099\u001b[0m             raw\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m   1100\u001b[0m             client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1101\u001b[0m             cast_to\u001b[38;5;241m=\u001b[39mextract_response_type(response_cls),\n\u001b[1;32m   1102\u001b[0m             stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1103\u001b[0m             stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1104\u001b[0m             options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1105\u001b[0m             retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1106\u001b[0m         ),\n\u001b[1;32m   1107\u001b[0m     )\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cast_to \u001b[38;5;241m==\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, response)\n",
      "File \u001b[0;32m~/anaconda3/envs/tserie/lib/python3.10/site-packages/openai/_base_client.py:1064\u001b[0m, in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "# Import the pipeline that calls the LLM\n",
    "from utils.pipeline import run_pipeline\n",
    "\n",
    "# Configurable parameter: top_k (must be less than 15)\n",
    "TOP_K = 1  # Adjust as needed\n",
    "\n",
    "# Paths\n",
    "TESTSET_CSV_PATH = \"/home/jaf/battery-lifespan-kg/eval/evaluator_question_short.csv\"  # Update if needed\n",
    "BATTERY_FILES_DIR = \"/home/jaf/battery-lifespan-kg/resources/testset\"\n",
    "OUTPUT_DIR = \"/home/jaf/battery-lifespan-kg/eval\"\n",
    "\n",
    "def save_llm_responses():\n",
    "    # Read the testset CSV\n",
    "    df = pd.read_csv(TESTSET_CSV_PATH)\n",
    "    \n",
    "    # Identify and sort sample question columns (e.g., SAMPLE_QUESTION_1, SAMPLE_QUESTION_2, ...)\n",
    "    sample_question_cols = sorted(\n",
    "        [col for col in df.columns if col.startswith(\"SAMPLE_QUESTION\")],\n",
    "        key=lambda x: int(x.split('_')[-1])\n",
    "    )\n",
    "    \n",
    "    output_rows = []\n",
    "    \n",
    "    # Iterate over each test case\n",
    "    for idx, row in df.iterrows():\n",
    "        test_battery_id = row[\"TEST_BATTERY_ID\"]\n",
    "        test_battery_query_feature = row[\"TEST_BATTERY_QUERY_FEATURE\"]\n",
    "        battery_file_path = os.path.join(BATTERY_FILES_DIR, f\"{test_battery_id}.txt\")\n",
    "        \n",
    "        # Read the battery file content\n",
    "        try:\n",
    "            with open(battery_file_path, \"r\") as f:\n",
    "                file_content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file for battery {test_battery_id}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare the row data dictionary with base columns\n",
    "        row_data = {\n",
    "            \"TEST_BATTERY_ID\": test_battery_id,\n",
    "            \"TEST_BATTERY_QUERY_FEATURE\": test_battery_query_feature\n",
    "        }\n",
    "        \n",
    "        # For each sample question, obtain the raw LLM response\n",
    "        for i, question_col in enumerate(sample_question_cols, start=1):\n",
    "            base_question = str(row[question_col]).strip()\n",
    "            \n",
    "            # Append instruction to force the list format with top-K results\n",
    "            modified_question = (\n",
    "                f\"{base_question} return the top-{TOP_K} results from the most similar first. \"\n",
    "                \"Please respond with a comma-separated list of battery IDs only.\"\n",
    "            )\n",
    "            \n",
    "            # Create a new file-like object for each call (to avoid pointer issues)\n",
    "            uploaded_file = StringIO(file_content)\n",
    "            \n",
    "            try:\n",
    "                # Run the pipeline with the modified question and the file content\n",
    "                response = run_pipeline(modified_question, uploaded_file)\n",
    "                if isinstance(response, str):\n",
    "                    raw_response = response\n",
    "                else:\n",
    "                    raw_response = response.get(\"result\", \"\")\n",
    "            except Exception as e:\n",
    "                raw_response = f\"API error: {str(e)}\"\n",
    "                print(f\"Error processing battery {test_battery_id} question '{base_question}': {str(e)}\")\n",
    "            \n",
    "            # Save the original question and its raw response in the row data\n",
    "            row_data[f\"q{i}\"] = base_question\n",
    "            row_data[f\"q{i}_raw_response\"] = raw_response\n",
    "        \n",
    "        output_rows.append(row_data)\n",
    "    \n",
    "    # Create a DataFrame from the collected rows\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    \n",
    "    # Construct the output CSV path using the input CSV name\n",
    "    input_csv_name = os.path.basename(TESTSET_CSV_PATH)\n",
    "    output_csv_path = os.path.join(OUTPUT_DIR, f\"llm_response_for_{input_csv_name}.csv\")\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    output_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"LLM responses saved to {output_csv_path}\")\n",
    "\n",
    "save_llm_responses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tserie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
